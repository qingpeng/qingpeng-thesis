\chapter{Review of relevant literature}


\section{Challenges in counting k-mers accurately and efficiently}

A k-mer is a substring with length k in a DNA sequence. K-mer counting is the
problem to determine the occurrences of such k-mers in a DNA dataset 
\cite{Marcais2011}. Efficient k-mer counting plays an important role in solving many
bioinformatics problems. 

One important problem is \textit{de novo} assembly of very large number of short reads.
With the development of next generation sequencing(NGS) technology, many research
groups can afford the sequencing of the sample of specific species or even
metagenomic samples with numerous different species\cite{pubmed19997069}. 
Large amount of NGS short reads are generated and de novo assembly is
required for these sequence data sets\cite{pubmed20211242}. Currently, de Bruijn graph method is
popular in the attempts to do \textit{de novo} assembly because of its advantage in
assembling next generation sequencing short reads\cite{Pevzner2001}. Several
popular assemblers based on de Bruijn graph have been published , including 
Velvet\cite{Zerbino2008}, ALLPATHS\cite{Butler2008}, ABySS\cite{Simpson2009}
and SOAPdenovo\cite{Li2010}. All the k-mers in a sequence
data set are represented as nodes in the de Bruijn graph. If
two k-mers have an overlap of (k-1)-mer, the two k-mers can be connected. Since
k-mer is such a basic unit in \textit{de Bruijn} graph \textit{de novo}
assembly, it is of
great importance to determine the occurrence of k-mers. One example is that
sequencing errors can generate many erroneous unique k-mers and we can filter
out the reads with too many unique k-mers before doing the assembly. Similarly,
we can also filter out the reads with k-mers that occur too many times for
smoothing MDA-abundance reads dataset. Pre-filtering reads to reduce the size
of reads data set to assemble is important to reduce the time and memory usage.
Another application of k-mer counting is the evaluation of microbial diversity
in metagenomic samples. Counting the number of k-mers and geting the k-mer
abundance distribution can give us some hints about the richness and evenness
of a metagenomic sample, although the erroneous k-mers from sequencing error
will cause some problems. Also, one of the popular approaches to do metagenomic
contigs binning is based on analyzing the k-mer abundance profile 
\cite{Patil:2012aa} \cite{Brady:2011aa}\cite{Rosen:2011aa}.Actually this is 
one of our motivations of the development of khmer, which
will be discussed in next chapter. One more example of applications of 
k-mer counting is the de novo detection of repetitive
elements. K-mer frequency can give important information for predicting regions
with such repetitive elements as transposons with important biological
function.\cite{Kurtz2008}

There are two specific characteristics of NGS shot reads that make k-mer
counting complicated. One is that large size of NGS reads data means there are
large number of
k-mers to count. However the large number of k-mers in a NGS reads data set
still only account for a small proportion of the total number of possible k-mers.
For a typical value of k as 20, there are $4^{20}$ possible k-mers, which are
far more than the actual number of k-mers present in any genome reads data set.
Delicate choice of data structure to use is desired to accommodate to such
sparseness of actual k-mers to enable efficient counting. Also, out of the
large number of k-mers in a NGS reads data set, many of them, especially unique
ones, are erroneous because of sequencing errors. Generally one sequencing error will
introduces $k$ erroneous k-mers. With a relatively high error rate as ~0.1-1\%
in Illumina reads data \cite{pubmed19997069}), as we do more sequencing and have
more reads, there will be actually more erroneous k-mers than true k-mers in
the data set. Effective and efficient methods to count the large number of
k-mers with most of them as erroneous is highly required in NGS reads data
analysis \cite{Minoche2011}.

Current methods to do k-mer counting involve the data structures like hash
tables, suffix arrays, binary trees or tries structures. If the size of
sequence dataset to count is modest, a simple hash table will suffice,
where the key is the k-mer and the value is the corresponding count. However
there is an obvious obstacle. If the size of sequence dataset is larger, the
efficiency of the counting using a simple hash table drops dramatically.
Instead another k-mer counting tool - Tallymer, uses suffix array data
structure\cite{Kurtz2008}. Admittedly it is more efficient than simple hash table in
general. However, the memory requirement is still linear to the number of
unique k-mers. Thus this method is not very scalable. For example,
the size of a soil metagenomic data set for one sample only have already 
exceeded 400G bytes, with only a limited sequencing depth\cite{Howe2014}.
 Reads dataset with size like this is difficult for Tallymer to
handle. Jellyfish, \cite{Marcais2011} another popular k-mer counting tool,
uses updated hash table data structure, which can reduce the memory usage
to store k-mers and the "lock-free" feature of the hash table also enables the
parallelism of Jellyfish to make it more efficient. But it still has the same
problem as Tallymer â€“ linear increase of memory usage with respect to the
number of unique k-mers in the data set to count.

Many more k-mer counting software packages based on different data structure
and algorithms have been developed in recent years, including BFCounter, DSK, 
KMC, Turtle and KAnalyze \cite{Melsted2011, Rizk2013, Deorowicz2013, Roy2014, 
Audano2014}. These software packages differ with each other in algorithmic
trade-offs and functionality, especially on how to deal with the trade-off
between disk and memory usage, enabling online counting and retrieval or not,
exact count or not, and others. 
Table \ref{table:kmer_counting} shows a summary of most current k-mer counting
packages with the function and limitation for each.

% more discussion about bloom filter and count-min sketch

%
% This data structure used in khmer package which will be discussed in details
% in next chapter is based on a Count-Min Sketch \cite{Cormode2005}, a
% generalized probabilistic data structure for storing the frequency
% distributions of distinct elements. The implementation in khmer extends an earlier
% implementation of a Bloom filter \cite{Bloom70}, which has been previously
% used in bioinformatics applications, such as sequence matching
% \cite{DBLP:conf/padl/MaldeO09}, k-mer counting \cite{Melsted2011}, and de
% Bruijn graph storage and traversal \cite{Pell2012,Jones:2012aa}. Many other
% variations of Bloom filters have been proposed \cite{BroderM03}, including
% counting Bloom filters \cite{Fan:2000:SCS:343571.343572}, multistage filters
% \cite{DBLP:conf/sigcomm/EstanV02}, and spectral Bloom filters
% \cite{DBLP:conf/sigmod/CohenM03}, which are related to the Count-Min Sketch
% and our khmer implementation.

%%%%%% LANDSCAPE PAGES  %%%%%%
%% To produce graphics or tables in landscape mode,
%% begin by removing the "%" in the next two lines.
%\begin{landscape}
%\thispagestyle{empty}
%% The contents of the page can be centered using the center environment
%% or the \centering command. Insert either a table with the tabular environment
%% or input a graphics file.
%% Use \captionof{table}{caption_text} (or figure in place of table) 
%% to create the caption for a short table or a figure. 
%% Insert a long table in a table environment. It disables double spacing
%% which is permitted for long tables.  Finally remove the "%" from the next line.
%\end{landacape}

\begin{landscape}
\thispagestyle{empty}
%\begin{sidewaystable}[h]
%\tiny
\centering
\resizebox{21cm}{!} {
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\textbf{software}       & \textbf{algorithm}                                          & \textbf{\begin{tabular}[c]{@{}l@{}}(built in) get k-mer \\ occurrence histogram\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}(built in)specific \\ k-mer count retrieval?\end{tabular}} & \textbf{API?} & \textbf{multithreaded?} & \textbf{online counting?} \\ \hline
\textbf{BFCounter}      & bloom filter,filter out low abundance kmer                  & N                                                                                             & N                                                                                             & N             & Y                       & Y                         \\ \hline
\textbf{DSK}            & fixed-memory and fixed-disk space streaming algorithm       & Y                                                                                             & N                                                                                             & N             & N                       & N                         \\ \hline
\textbf{Jellyfish}      & lock-free hash table                                        & Y                                                                                             & Y                                                                                             & N             & Y                       & Y                         \\ \hline
\textbf{KAnalyze}       & split to disk and merge                                     & N                                                                                             & N                                                                                             & Y             & Y                       & N                         \\ \hline
\textbf{Khmer}          & count-min sketch                                            & Y                                                                                             & Y                                                                                             & Y             & Y                       & Y                         \\ \hline
\textbf{KMC}            & parallel disk-based,similar to DSK                          & N                                                                                             & N                                                                                             & N             & Y                       & N                         \\ \hline
\textbf{MSPKmerCounter} & Minimum Substring Partitioning                              & Y                                                                                             & Y                                                                                             & N             & Y                       & N                         \\ \hline
\textbf{Tallymer}       & enhanced suffix arrays                                      & Y                                                                                             & Y                                                                                             & N             & N                       & N                         \\ \hline
\textbf{Turtle}         & pattern-blocked Bloom filter, filter out low abundance kmer & N                                                                                             & N                                                                                             & N             & Y                       & Y                         \\ \hline
\end{tabular}
}
\label{table:kmer_counting}
\captionof{table}{\bf{Description of k-mer counting packages.}}
%\caption{\bf{Description of k-mer counting packages.}} 
 
\end{landscape}
%\end{sidewaystable}



\section{Tackling large and error-prone short-read shotgun data sets}

With the dramatic improvements of next generation sequencing(NGS) technologies
and the dropping sequencing cost, more research groups can afford 
sequencing the sample of specific species or even
metagenomic samples with numerous different species in large scale\cite{pubmed19997069}. 
This leads to the explosive growth of sequencing data to analyze. 
Important biological insights will be drawn from mining the large genomic data.
However serious challenges need to be overcome to enable efficient and effective
analysis. 

Similar to the k-mer counting problem, the first obstacle to analyze large NGS data
is the formidable size of 
the NGS data from high sequencing depth. Higher sequencing depth is 
required to assemble a genome successfully using NGS short reads than using 
longer reads from traditional Sanger sequencing technology.For example, to get
a decent coverage of human genome with by NGS short reads to get satisfying assembly, 
100x sequencing depth is generally required, which leads to a 300 GB NGS reads 
data set\cite{pubmed21187386}. For samples with variable abundance of genomic 
contents such as transcriptome or metagenome, to get enough sequencing depth for
the rarer genomic content, the overall sequencing depth will greatly increase, 
which leads to dramatically large reads data sets. However, with uneven
abundance of genomic contents meanwhile. 

Still like the k-mer counting problem, sequencing errors bring significant troubles
to effective and efficient analysis of large NGS data. With a relatively high 
error rate as ~0.1-1\% in Illumina reads data \cite{pubmed19997069}), the more 
sequencing we perform, the more sequencing errors we will have in the data, no 
matter if the sequencing depth is high or low. So for deep sequencing, most
of the novelty will be dominated by sequencing errors \cite{pubmed21245053}.
As discussed previously, generally one sequencing error will
introduce $k$ erroneous k-mers. So many sequencing errors will undermine the 
effectiveness of many down-streaming analysis of such short reads data.

With the two characteristics of NGS short reads data - large size and error-prone,
 combined with the dropped cost of sequencing, 
we are facing the third challenge, that the increase of our capacity to analyze data cannot
catch up with the capacity of generating data \cite{pubmed20441614}. It is difficult
to easily manipulate and analyze the large genomic data without significant improvement
of computational approaches.

The approaches to overcome these problems, are straightforward, in a way. To deal
with the large size of data, we try to decrease the size. To deal with the 
errors in data, we try to remove or correct them. 

Assembly can be seen as a solution to reduce the size of large data, with the 
sacrifice of losing abundance information of genomic contents. There have
been significant theoretical progress to store and analyze the big sequencing data
efficiently
\cite{pubmed22068540,pubmed20529929}. Based on the progress, a number of new 
assemblers have been developed,
such as ABySS, Velvet, SOAPdenovo, ALLPATHS, SGA, and Cortex
\cite{pubmed19251739,pubmed18349386,pubmed20511140,pubmed21187386,
pubmed22156294,Iqbal2012}. For sequencing data with uneven abundance of genomic contents
like metagenome or transcriptome, novel assembler or 3rd party add-ons are developed
specifically, such as Trinity, Oases, MetaVelvet, MetaVelvet-SL, Meta-IDBA, Velvet-SC, DIME, MEGAHIT, Omega
\cite{pubmed21572440,pubmed22368243,Namiki2012,Afiahayati2015, pubmed21685107,pubmed21926975,Guo2015,Li2015,Haider2014}.

Assembly can be seen as a solution to decrease error rate too. There may still
be errors in assembled contigs or genomes. However, large amount of errors located in 
numerous reads have been discarded in the procedure of assembly. 
To remove or correct errors directly from reads, k-mer spectral analysis is 
a popular approach\cite{Pevzner2001}. Basically low-abundance k-mers will be found and treated as 
likely errors. Those likely errors can be removed, trimmed with other k-mers, 
or corrected using statistical learning method \cite{Kelley2010}. More error
removal and correction software packages were developed\cite{Medvedev2011,pubmed15059830,Kelley2010}.
Such error removal or correction approaches can be applied to preprocess
reads data before assembly.

As necessary components in NGS data analysis pipeline, assembly and error removal/correction
have proved to be effective to enable more accurate interrogation to the large,
error-prone short-read data. However both assembly and error removal/correction are
very compute intensive. The memory usage of assembly does not scale well to the size
of data to assemble. The error removal/correction based on k-mer spectral analysis
normally involves two iterations of examining the data, which is also memory intensive
and time consuming. Streaming and semi-streaming algorithms, which examine the data 
only once or less than twice and scale well to the size of input data in memory/time 
requirements, are promising to be integrated with the pre-assembly preprocessing
and error removal/correction to achieve higher efficiency of NGS data analysis finally.
In this dissertation, we will discuss our efforts in this direction.




\section{Challenges in measuring diversity of metagenomics}


\subsection{Diversity measurement in microbial ecology}

There have been numerous mature methods and tools to measure diversity of
macroorganisms in decades of development of classic ecology. One would think
that we just need to borrow those methods to use in microbial field.
Unfortunately in reality this is not the case. The microbial communities are so
different from macroorganisms like plant or animal communities, with the number
of species many order of magnitude larger \cite{Whitman:1998aa}. This fact
raises serious sampling problems. It is extremely difficult to cover enough
fraction of the microbial community even with impressively large sample size
thanks to modern metagenomic approaches \cite{Roesch:2007aa}. In a word,
diversity measurement is a rather big challenge for microbial communities and
novel and effective methods are highly demanded \cite{Schloss:2005aa}.

\subsubsection{OTU Identification using sequence markers} To borrow the methods
of diversity measurement from classic ecology on the use of evaluating
microbial diversity, the first problem is that in microbial world, there is no
unambiguous way to define ``species'' \cite{Stackebrandt:2002aa}. It is
impossible to identify a microbial individual as a specific species
morphologically. In fact in metagenomics the concept of ``species'' has been
replaced by OTUs(Operational Taxonomic Units). An OTUs are those microbial
individuals within a certain evolutionary distance. Practically we mainly use
16S rRNA genes as the evolutionary marker genes, because 16S rRNA genes exist
universally among different microbial species and their sequences change at a
rate corresponding with the evolutionary distance. So we can describe microbial
individuals with higher than a certain percent(e.g. 97\%) 16S rRNA sequence
similarity as one OTU, or belonging to one species \cite{Schloss:2005aa}.

\subsubsection{Binning of metagenomic reads into OTUs}

In classic ecology dealing with samples from macroorganisms communities, before
we can use any statistical method to measure diversity, it is standard
procedure to identify the species of each individual in a sample. It is the
same for diversity measurement of microbial communities. Difference is that
here we need to place the sequences(individuals) into respective ``bin'' or
OTUs(species). There are two strategies to do such binning - Composition-based
or intrinsic binning approach and similarity-based or extrinsic binning
approach.

\paragraph{Composition-based approach} Lots of efforts have been put to get a
comprehensive category of reference microbial genome sequences \cite{HMScience,
Wu:2009aa}. Currently there are a large number of finished or high-quality
reference sequences of thousands of microbial species available in different
databases and this number is still increasing quickly \cite{Markowitz:2012aa,
Glass:2010aa, Wang:2007aa}. So the first intrinsic composition-based approach
is to use those reference genomes to train a taxonomic classifier and use that
classifier to classify the metagenomics reads into bins. Different statistical
approaches like Support Vector Machines \cite{Patil:2012aa}, interpolated
Markov models\cite{Brady:2011aa},naive Bayesian classifiers, and Growing Self
Organizing Maps \cite{Rosen:2011aa} were used to train the classifier. Without
using any reference sequences for the training, it is possible to use
signatures like k-mers or codon-usage to develop reference-independent
approach. The assumption is that the frequencies distribution of the signatures
are similar of the sequences from the same species. TETRA is such a
reference-independent tools using Markov models based on k-mer frequencies
\cite{Teeling:2004aa}. There is another tool integrating TETRA and the codon usage
profile to classify reads \cite{Tzahor:2009aa}.


\paragraph{Similarity-based approach} The similarity-based extrinsic approach
is to find similarity between the reads sequences and reference sequences and a
tree can be built using the similarity distance information. MEGAN
\cite{Huson:2007aa} is a typical tool using this method, which reads a BLAST
file output. Other sequence alignment tools can also be used here like BowTie2
or BWA. Recently, an alternative strategy was developed, which only uses the
reference sequences with the most information rather than all the reference
sequences to do alignment. Those reference sequences include 16S rRNA genes or
some other specific marker genes. The benefit is obvious, it is more
time-efficient since there are fewer reference sequences to align to. Also, it
can provide better resolution and binning accuracy since the marker genes can
be selected carefully with the best distinguishing power. AMPHORA2
\cite{Wu:2012aa} and MetaPhlAn \cite{Segata:2012aa} are two typical tools using
this strategy.



\subsubsection{Statistics for diversity estimation} After the binning of
sequences into OTU, we need statistical analysis to help us estimate the diversity. Many
statistical methods have been developed and widely used in classical ecology of
macroorganisms. However the first difference between diversity measurement of
macroorganisms and microbial community is that generally the microbial
community diversity is much larger than observed sample diversity, thanks to
the high diversity characteristics of microbial community and the limit of
metagenomics sampling and sequencing. The first approach which is also
considered as classic is rarefaction. Rarefaction curve can be used to compare
observed richness among different samples that have been sampled unequally,
which is basically plotting of the relationship between the number of observed species 
and the number of sampled individuals. It is worth noting that rarefaction curve shows the
observed diversity, not the total diversity. We should not disregard those
unseen microbial species, which is pretty common for microbial community
sampling.

To estimate the total diversity from observed diversity, different estimators
are required.

The first one is extrapolation from accumulation curve. The asymptote of this
curve is the total diversity, which means the number of species will not
increase any more with sampling more individuals. To get the value of that
asymptote point, from observed accumulation curve, a function needs to be
assumed to fit the curve. Several proposals have been made to use this
extrapolation method \cite{colwell2004interpolating, gotelli2001quantifying}.
The problem is that if the sampling effort only covers a small fraction of the
total sample, which means the accumulation curve just starts, it is difficult
to find an optimal function to fit the curve. Different functions can fit the
curve equally well but will deduct dramatically different asymptote value. So
this curve extrapolation method should be used cautiously.

Another one is parametric estimator, which assumes that the relative abundance
follows a particular distribution. Then the number of species unobserved in the
community can be estimated by fitting observed sample data to such abundance
distribution then the total number of species in the community can be
estimated. Lognormal abundance distribution is mostly used in different project
since most communities of macroorganisms has a lognormal abundance distribution
and it is believed that it is also typical for some microbial communities
\cite{Curtis:2002aa, Schloss:2006aa, Quince:2008aa}. It is understandable that
 it is difficulty to know which models fit the communities best since
in an ideal world the abundance distribution should be inferred from the
data,not be assumed unverifiably. The problem is that we can only infer the
abundance distribution accurately when the sample size is large enough. There
have been some attempts on this direction recently \cite{Gans:2005aa} and more
robust methods are still needed.

If the species abundance distribution cannot be inferred, we can still use
nonparametric estimators to estimate the total diversity without assuming that
abundance distribution arbitrarily. These estimators are related to
MRR(mark-release-recapture) statistics, which compare the number of species
observed more than once and the number of species observed only once. If
current sampling only covers a small fraction of a diverse community, most
species will be observed only once. If current sampling is enough to cover most
species in the community, the opposite will be the case. A series of estimators
invented by Chao are the representative estimators in this category, including
Chao1 \cite{chao1984nonparametric}, Chao2 \cite{Chao:1987aa}, ACE
\cite{chao1993stopping} and ICE \cite{lee1994estimating}. For example, Chao1
formular is:\\ $${S}_{Chao1}={S}_{obs}+\frac{{{n}_{1}}^{2}}{2{n}_{2}}$$ where
${S}_{obs}$ is the number of species observed, ${n}_{1}$ the number of species
observed once(singletons, with only one individule), and ${n}_{2}$ the number
of species observed twice(doubletons, with exactly two individuals) in the
sample. The ACE uses data from all species rather than just singletons and
doubletons. Its formular is:\\
$${S}_{ACE}={S}_{abund}+\frac{{S}_{rare}}{{C}_{ACE}}+\frac{{F}_{1}}{{C}_{ACE}}{
{\gamma }_{ACE}}^{2}$$ where ${S}_{rare}$ is the number of rare species (with
few than 10 observed individuals) and ${S}_{abund}$ is the number of abundant
species (with more than 10 observed individuals).

In past years there are several software packages that have been developed for
biodiversity analysis. Out of them, EstimateS \cite{colwellestimates} is a
software that can be used for general purpose diversity analysis, which
implement a rich set of diversity analysis algorithms. However it is not
designed specifically for microbial diversity analysis. So microbial diversity
data should be preprocessed to general population data to be fed into
EstimateS. Two other softwares - MOTHUR \cite{Schloss:2009aa} and QIIME
\cite{Caporaso:2010aa} are designed for microbial diversity. So they are more
popular in microbial diversity analysis. CatchAll \cite{Bunge:2011aa} is a
relatively newer package, which can estimate the diversity using both
nonparametric and parametric estimators including many variants and return the
results using different estimators and the respective credibility of the
results.


